{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "## 7 Convelutional Neural Networks\n",
    "### 7.1 From Fully Connected Layers to Convolution\n",
    "\n",
    "Lets say we have a fully labeled collection of one-megapixel photographs of cats and dogs to distinguish. This is $10^6$ x $10^3$ = $10^9$ parameters. This is an unfeasable amount of parameters to learn.\n",
    "As such we employ convolutional neural networks as a way to exploit the known structure in natural images\n",
    "\n",
    "Take Where's Waldo, what Waldo looks like does not depend on Waldo's location. We could break the image into patches and assign a score to each patch, and CNNs systematize this idea of spacial invariance.\n",
    "\n",
    "We can now make intuitions:\n",
    "In earlier layers, our network should respond similarly to the same patch, regardless of where it is in the images, focusing on local reigions. Deeper layer should aim to capture longer rage features of the image.\n",
    "\n",
    "We start by representing the image X and intermediate hidden representation H as matrices, both with the same shape. X[i,j] and H[i,j] denotes pixel location.\n",
    "\n",
    "H[i,j] = U[i,j] + sum_a( sum_b( V[i,j,a,b] X[i+a, j+b])) Where U contains biases and V is weights. becase we have i+a and j+b we can run over positive and negative offsets centered around i,j.\n",
    "\n",
    "For this single layer we now need $10^12$ parameters.\n",
    "\n",
    "First Principle: Translation invariance means that a shift in X should result in a shift in H, which is only possible if V and U do not depend on [i,j]. We simplyify U to constant u and remove i and j from V:\n",
    "\n",
    "H[i,j] = u + sum_a( sum_b( V[a,b] X[i+a, j+b])) This is a convolution, as we are weight pixels at i+a and j+b to obtain H[i,j] using V[a,b]. As we remove V depending on the location and simplifying U, we reduce the number of paramters from $10^12$ to 4 x $10^6$\n",
    "\n",
    "Second Principle: Locality, we should not have to look far from [i,j] to glean relegant information, as such outputide some range |a| > $\\Delta$ or |b| > $\\Delta$ V[a,b] = 0.\n",
    "\n",
    "\n",
    "H[i,j] = u + sum[a = - $\\Delta$ to $\\Delta$]( sum[b = - $\\Delta$ to $\\Delta$]( V[a,b] X[i+a, j+b]))\n",
    "This reduces parameters from 4 x $10^6$ to 4 $\\Delta^2$, where $\\Delta$ is typically smaller than 10.\n",
    "\n",
    "With this we have reduced an image from billions of paramaeters to just several hundred, without altering dimensionality. However now this exclusivly works on local areas, we must interleave nonlinearities and convolutional layers repeatedly to account for deeper layers needing to generalise the entire image.\n",
    "\n",
    "However this is entirely based on non-colour images, if we include 3 channel colours instead of V[a,b] we now have V[a,b,c]. Additionally we want an entire vector of hidden representations to correspond to each spatial location. So we now have\n",
    "\n",
    "H[i,j] = u + sum[a = - $\\Delta$ to $\\Delta$]( sum[b = - $\\Delta$ to $\\Delta$]( sum[c]( V[a,b,c,d] X[i+a, j+b, c])))\n",
    "\n",
    "where d index the output channels in hidden representations H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Convolutions for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a matrix and multiply by kernal gives new matrix size n[h] x n[w] minus convolution kernel k[h] x k[w]:\n",
    "# (n[h] - k[h] + 1) x (n[w] - k[w] + 1)\n",
    "\n",
    "def corr2d(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce and output, inputs of a convolutional layer are kernel and scalar bias\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is a simple application, detecting the edge of an object by finding the first pixel change\n",
    "\n",
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel with height 1 and width 2, if horizontally adjacent elements are the same, output is 0, otherwize non-zero\n",
    "\n",
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 from white to black, -1 from black to white\n",
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we apply the kernel to transposed image we get nothing as it cannot detect vertical edges\n",
    "corr2d(X.t(), K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 12.637\n",
      "epoch 4, loss 2.913\n",
      "epoch 6, loss 0.813\n",
      "epoch 8, loss 0.270\n",
      "epoch 10, loss 0.100\n"
     ]
    }
   ],
   "source": [
    "# this system is greate if we know what we are looking for, however at larger kernels and sucessive convolutions, it may be impossible to precisly specify what each filter should be doing.\n",
    "\n",
    "# attemping to learn the kernel\n",
    "\n",
    "\n",
    "# two dimensional convolutional layer, kernel shape 1x2\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# four dimensional input, 1 dimensional output\n",
    "# (batch size, channel, height, width), batch size and channels are 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "# learning rate\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # update kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0155, -0.9527]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Padding and Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we start off with a 240x240 image, 10 layers of 5x5 convolutions, we reduce the image too 200x200\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can pad a 3x3 input to 5x5, and output a 4x4 matrix if we use a 2x2 kernel. As such we use kenerls with odd width and height to retain dimensionality after padding\n",
    "\n",
    "# here is a convolutional layer with height and width of 3, apply 1 pixel padding. Given input with height and width of 8, we find output is also 8\n",
    "def comp_conv2d(conv2d, X):\n",
    "    X = X.reshape((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # strip batch size and channels\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# padding 1 row/column each side\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kernel height 5, width 3\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of moving 1 by 1 by 1 pixel each time, we could skip a pixel or two, this is defined by stride. This will decrease the dimensionality of the output\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape\n",
    "# in this case input height and width is halved as a result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Multiple Input and Multiple Output Channcels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously we just considered multiple input and output as multiple single input and output. This allowed us to represent data as two-dimensional tensors\n",
    "# Adding additional channels transforms it into tree-dimensional tensor. We will look at convolutional kernels with multiple input and output channels\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need our kernal to have the same number of channels as input. \n",
    "# we use this to perform cross-correlation with input data\n",
    "\n",
    "def corr2d_multi_in(X, K):  # perform cross correlation per channel and add them up.\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is essential to have multiple channels at each layer. In most popular models we increase channel dimension as we go deeper.\n",
    "# the shape of the kernel is co x ci x kh x kw. co and ci are input and output channels, kh and kw are height and width of kernel\n",
    "\n",
    "def corr2d_multi_in_out(X, K):\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triviual kernel with three output channels\n",
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 x 1 kernel\n",
    "\n",
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))\n",
    "    K = K.reshape((c_o, c_i))\n",
    "    Y = torch.matmul(K, X) # matrix multiplication in fully connected layer\n",
    "    return Y.reshape((c_o, h, w))\n",
    "\n",
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooling layers serves the dual purpose of mitigating the sensetivity of convolutional layers to location, and of spacially downsampling representations\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average pooling takes the average of the elements in the pooling window and reduces the dimensionality\n",
    "# max pooling takes the maximum of all the elements in the pooling window\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as with convolutional layers, we can adjust the shape through padding and stride\n",
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with a pooling window of (3, 3), we get a stride shape of (3, 3)\n",
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can have arbitary rectangular pooling windows too\n",
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for multiple layers it processes each layer individually\n",
    "X = torch.cat((X, X + 1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights\n",
    "def init_cnn(module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "# LeNet 5 model\n",
    "class LeNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of all the layer sizes\n",
    "@d2l.add_to_class(d2l.Classifier)\n",
    "def layer_summary(self, X_shape):\n",
    "    X = torch.randn(*X_shape)\n",
    "    for layer in self.net:\n",
    "        X = layer(X)\n",
    "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "model = LeNet()\n",
    "model.layer_summary((1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training LeNet\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = LeNet(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Networks Using Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the design of nerual network architectures has grown more abstract, moving from individual neurons to layers, and now to blocks\n",
    "# a CNN is a sequence of convolutional layers, nonlinear activation functions, and pooling layers. One of the problems with this is that the spatial resolution decreases quite rapidly.\n",
    "# One key idea is to use multiple convolutions in between downsampling, i.e. two 3x3 convolutions touches the same amount of pixels as a single 5x5 convolution does, but with fewer parameters. Deep narrow networks out perform shallow wide networks.\n",
    "\n",
    "# VGG consists of a sequence of convolutions with 3x3 layers with padding of 1 to keep height and weidth. Followed by a 2x2 max-pooling layer with stride of 2, halving height and width\n",
    "# this code is to implement one VGG block\n",
    "\n",
    "def vgg_block(num_convs, out_channels):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one major advantage is that it is primarily composed of these blocks, compared to other models where each layer is unique\n",
    "# VGG defines a family of networks rather than a specific manifestation, and we can modify the number of convolutional layers and output channels.\n",
    "class VGG(d2l.Classifier):\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        conv_blks = []\n",
    "        for (num_convs, out_channels) in arch:\n",
    "            conv_blks.append(vgg_block(num_convs, out_channels))\n",
    "        self.net = nn.Sequential(\n",
    "            *conv_blks, nn.Flatten(),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.LazyLinear(num_classes))\n",
    "        self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:\t torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:\t torch.Size([1, 25088])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# the original VGG network had 5 convolutional blocks, the first two with one convolutional layer each, and the remaining three with two convolutional layers each.\n",
    "# the first block has 64 output channels and each subsequent block doubles the number of output channels, up to 512.\n",
    "VGG(arch=((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))).layer_summary(\n",
    "    (1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are training here with a smaller number of channels to decrease computation time\n",
    "\n",
    "model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Residual Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know how adding layers can increase complexity and expressibeness of the network, and design networks where adding layers makes networks more expressive rather than just different.\n",
    "\n",
    "If larger function classes contain smaller ones, we are guaranteed that increasing them strictly increases expressive power of a network. For deep neural networks, if we can train the newly added layer into an identiy function, the new model will be just as effective as the original\n",
    "\n",
    "This leads to the discovery of a residual block. If we have f(x) where x is the input, the activation function must directly learn from x, however if we take the residual mapping g(x) = f(x) - x we may be able to learn better, with the ideal of f(x) = x or g(x) = 0. As such we only need to push weights and biases of the upper weight layer to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet has VGG's 3x3 design, where the residual block has two 3x3 convolutional layers. Each *convolutional* layer is followed by a batch normalization layer and ReLU activation function. Then, we skip these operations and add the input directly before the final ReLU activation function. This means the input and output must be the same shape\n",
    "\n",
    "# the residual block\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where 1x1 convolution is not needed\n",
    "blk = Residual(3)\n",
    "X = torch.randn(4, 3, 6, 6)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where 1x1 convolution is needed, halve output height and width while increasing output channels\n",
    "blk = Residual(6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7x7 convolutional layer with 64 output channels and stride of 2, followed by a 3x3 max pooling layer with stride of 2\n",
    "class ResNet(d2l.Classifier):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four modules made up of residual blocks\n",
    "# retains the same height and width, but doubles the number of channels\n",
    "@d2l.add_to_class(ResNet)\n",
    "def block(self, num_residuals, num_channels, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average pooling layer followed by fully connected layer output\n",
    "@d2l.add_to_class(ResNet)\n",
    "def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b1())\n",
    "    for i, b in enumerate(arch):\n",
    "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "    self.net.add_module('last', nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)))\n",
    "    self.net.apply(d2l.init_cnn)\n",
    "# there are four convelutional layers in each module, with the first 7x7 convolutional layer and final fully connected layer, there are 18 layers in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape changes\n",
    "class ResNet18(ResNet):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
    "                       lr, num_classes)\n",
    "\n",
    "ResNet18().layer_summary((1, 1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model = ResNet18(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNeXt splits the input into multiple paths/groups. In Resnet, no information is exchanged between groups, and the output is the sum of outputs for all groups. ResNeXt amends this by using 3x3 kernels sandwitched between two 1x1 convolutions\n",
    "\n",
    "# this implementation takes argument groups g, with bot_channels b\n",
    "class ResNeXtBlock(nn.Module):\n",
    "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
    "                 strides=1):\n",
    "        super().__init__()\n",
    "        bot_channels = int(round(num_channels * bot_mul))\n",
    "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
    "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
    "                                   stride=strides, padding=1,\n",
    "                                   groups=bot_channels//groups)\n",
    "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.bn3 = nn.LazyBatchNorm2d()\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "            self.bn4 = nn.LazyBatchNorm2d()\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.conv4:\n",
    "            X = self.bn4(self.conv4(X))\n",
    "        return F.relu(Y + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blk = ResNeXtBlock(32, 16, 1)\n",
    "X = torch.randn(4, 32, 96, 96)\n",
    "blk(X).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
