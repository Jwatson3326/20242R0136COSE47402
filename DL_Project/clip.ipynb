{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_prompts = [\n",
    "    \"This image contains text in English\",\n",
    "    \"This image contains text in Spanish\",\n",
    "    \"This image contains text in French\",\n",
    "    \"This image contains text in Korean\",\n",
    "    \"This image contains text in Chinese\",\n",
    "    \"This image contains text in Arabic\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Preprocess the image for CLIP\n",
    "    return processor(images=image, return_tensors=\"pt\", padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(image_path, language_prompts):\n",
    "    inputs = preprocess_image(image_path)\n",
    "\n",
    "    # Generate image embeddings\n",
    "    image_features = model.get_image_features(**inputs)\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    # Generate text embeddings\n",
    "    text_inputs = processor(text=language_prompts, return_tensors=\"pt\", padding=True)\n",
    "    text_features = model.get_text_features(**text_inputs)\n",
    "    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute similarity between image and text embeddings\n",
    "    similarities = (image_features @ text_features.T).squeeze()\n",
    "\n",
    "    # Identify the most similar language\n",
    "    best_match_idx = similarities.argmax().item()\n",
    "    return language_prompts[best_match_idx], similarities[best_match_idx].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: This image contains text in English\n",
      "Confidence: 0.20702892541885376\n"
     ]
    }
   ],
   "source": [
    "image_path = \"sign.jpg\"  # Replace with your image path\n",
    "detected_language, confidence = detect_language(image_path, language_prompts)\n",
    "print(f\"Detected Language: {detected_language}\")\n",
    "print(f\"Confidence: {confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€˜ 19 91 77 183 0\n",
      "o 99 108 129 137 0\n",
      "e 58 86 275 183 0\n",
      "i 99 2 200 137 0\n",
      "T 46 46 69 90 0\n",
      "t 64 43 75 90 0\n",
      "0 83 56 109 95 0\n",
      "4 118 39 124 60 0\n",
      "3 127 42 137 65 0\n",
      "0 139 48 150 71 0\n",
      "0 153 54 164 77 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "# Specify the path to the tesseract executable\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "def extract_text_regions(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    text_regions = pytesseract.image_to_boxes(gray)  # Get bounding boxes\n",
    "    # Crop text regions and return\n",
    "    return text_regions\n",
    "# USE THIS IN PIPELINE TO CROP OUT PARTS WITH TEXT THEN PASS IN, USE COORDS TO DRAW WITH LANGUAGE????\n",
    "print(extract_text_regions(image_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
